from pathlib import Path
import os
import pandas as pd


#
# def gen_sample_postfix_list(sample_list, demultiplexing_matrix_file_list):
#     sample_postfix_list = []
#     for sample, matrix_file in zip(sample_list, demultiplexing_matrix_file_list):
#         df = pd.read_csv(matrix_file,header=None,names=["Name", "Sequence"], sep="\t")
#         post_fixes = set(df["Name"])
#         for pf in post_fixes:
#             sample_postfix_list.append(f"{sample}_R1_{pf}")
#             sample_postfix_list.append(f"{sample}_R2_{pf}")
#     return sample_postfix_list
# def gen_sample_postfix_dict(sample_list, demultiplexing_matrix_file_list):
#     sample_postfix_dict = {}
#     for sample, matrix_file in zip(sample_list, demultiplexing_matrix_file_list):
#         df = pd.read_csv(matrix_file,header=None,names=["Name", "Sequence"], sep="\t")
#         post_fixes = set(df["Name"])
#         sample_postfix_dict[sample] = list(post_fixes)
#     return sample_postfix_dict
#
# def gen_sample_postfix_match(wildcards):
#     return  pd.read_csv(os.path.join(wrkdir,symlink_dir,f"{wildcards.sample}_index.tsv"),header=None,names=["Name", "Sequence"],sep="\t")["Name"].tolist()
#


wrkdir = Path(config["wrkdir"])
log_dir = Path(config["logdir"])
split_count = config["split_count"]
sample_name = config["sample_name"]
index_file = config["index_file"]
indices_names = config["indices_names"]
metadata_file = config["metadata_file"]

tmp_dir = "tmp"
split_dir = "splits"
demultiplex_dir = "demultiplexed"
# merged_dir = "demultiplexed_merge"
symlink_dir = "symlinks"

# sample_postfix_name_list = gen_sample_postfix_list(sample_list=sample_list, demultiplexing_matrix_file_list=demultiplexing_matrix_file_list)
# sample_postfix_name_dict = gen_sample_postfix_dict(sample_list=sample_list, demultiplexing_matrix_file_list=demultiplexing_matrix_file_list)




container: "docker://condaforge/mambaforge"


rule all:
    input:
        expand(os.path.join(wrkdir,demultiplex_dir, "{sample_name}_{Read}_{indices_names}.fastq.gz"), sample_name=sample_name, indices_names=indices_names, Read=["R1", "R2"])

rule create_links:
    input:
        metadata=metadata_file,
    params:
        alignment_dir=os.path.join(wrkdir, tmp_dir, symlink_dir),
    output:
        r1 = os.path.join(wrkdir, tmp_dir, symlink_dir, "{sample}_R1.fastq.gz"),
        r2 = os.path.join(wrkdir,tmp_dir, symlink_dir,"{sample}_R2.fastq.gz"),
        index_file = os.path.join(wrkdir,tmp_dir, symlink_dir,"{sample}_index.tsv")
    resources:
        mem_mb=1000,
        runtime=20,
        nodes=1,
    threads: 1
    run:
        df_metadata = pd.read_csv(input.metadata)
        df_metadata = df_metadata[df_metadata["Sample_Name"].isin(sample_name)]
        os.makedirs(os.path.join(wrkdir, symlink_dir), exist_ok=True)
        for index, row in df_metadata.iterrows():
            # Fastq File
            fastq_file = Path(row["Absolute_Path"])
            output_file_Read = os.path.join(wrkdir, symlink_dir, f'{row["Sample_Name"]}_{row["Read"]}.fastq.gz')
            if os.path.exists(output_file_Read):
                os.remove(output_file_Read)
            os.symlink(fastq_file, output_file_Read)
            # Index File
            index_file = Path(row["Absolute_Path_index_file"])
            output_file_index = os.path.join(wrkdir, symlink_dir, f'{row["Sample_Name"]}_index.tsv')
            if os.path.exists(output_file_index):
                os.remove(output_file_index)
            os.symlink(index_file, output_file_index)




rule fastqsplitter:
    input:
        read = os.path.join(wrkdir,tmp_dir, symlink_dir,"{sample}.fastq.gz"),
    output:
        splitt_files=expand(
            os.path.join(wrkdir,tmp_dir,split_dir,"{{sample}}_Split_{split_no}.fastq.gz"),
            split_no=["00" + str(i) if i > 9 else "000" + str(i) for i in range(split_count)]),
    params:
        split_count=split_count,
        output_list =  "--output " + " --output ".join([os.path.join(wrkdir,tmp_dir,split_dir,"{{sample}}_Split_{split_no}.fastq.gz")
                                                        for split_no in [
                "00" + str(i) if i > 9 else "000" + str(i)
                for i in range(split_count)
            ]]),
    conda:
        "../envs/demultiplex.yml"
    threads:
        split_count
    log:
        os.path.join(wrkdir, log_dir, "fastqsplitter", "{sample}.log")
    resources:
        mem_mb=2000,
        runtime=7*24*60,
        node=1,
    shell:
        "fastqsplitter "
        "--input {input.read} "
        "--compression-level 1 "
        "--threads-per-file 1 "
        "{params.output_list} "
        "&> {log}"


rule demultiplexing:
    input:
        read_1 = os.path.join(wrkdir,tmp_dir,split_dir,"{sample}_R1_Split_{split_no}.fastq.gz"),
        read_2 = os.path.join(wrkdir,tmp_dir,split_dir,"{sample}_R2_Split_{split_no}.fastq.gz"),
        demultiplexing_matrix_file = os.path.join(wrkdir,tmp_dir,symlink_dir,"{sample}_index.tsv")
    output:
        # read_1 = expand(os.path.join(wrkdir,tmp_dir,demultiplex_dir, "{sample}", "{sample}_R1_Split_{split_no}_{postfix}.fastq.gz"), postfix=lambda wildcards: gen_sample_postfix_match(wildcards=wildcards)),
        read_1 = expand(os.path.join(wrkdir,tmp_dir,demultiplex_dir, "{{sample}}", "{{sample}}_R1_Split_{{split_no}}_{postfix}.fastq.gz"), postfix=indices_names),
        read_2 = expand(os.path.join(wrkdir,tmp_dir,demultiplex_dir, "{{sample}}", "{{sample}}_R2_Split_{{split_no}}_{postfix}.fastq.gz"), postfix=indices_names),
        # read_2 = expand(os.path.join(wrkdir,tmp_dir,demultiplex_dir,"{sample}", "{sample}_R2_Split_{split_no}_{postfix}.fastq.gz"), postfix=lambda wildcards: gen_sample_postfix_match(wildcards=wildcards)),
        # outdir = directory(os.path.join(wrkdir,tmp_dir,demultiplex_dir, "{sample}"))
    params:
        new_demultiplexing_matrix_file = os.path.join(wrkdir,tmp_dir,demultiplex_dir, "demultiplex_matrix_file_{sample}_{split_no}.tsv")
    conda:
        "../envs/demultiplex.yml"
    threads:
        1
    log:
        os.path.join(wrkdir, log_dir, "demultiplex", "{sample}_{split_no}.log")
    resources:
        mem_mb=4000,
        runtime=7*24*60,
        node=1,
    shell:
        "cp {input.demultiplexing_matrix_file} {params.new_demultiplexing_matrix_file}; "
        "demultiplex demux "
        "-p {output.outdir} "
        "{params.new_demultiplexing_matrix_file} "
        "{input.read_1} "
        "{input.read_2} "
        "&> {log}"


rule merge_demultiplexed_files:
    input:
        read = expand(
            os.path.join(wrkdir,tmp_dir,demultiplex_dir, "{{sample}}", "{{sample}}_{{Read}}_Split_{split_no}_{{postfix}}.fastq.gz"),
            split_no=[
                "00" + str(i) if i > 9 else "000" + str(i)
                for i in range(split_count)
            ],
        )
    output:
        read = os.path.join(wrkdir,demultiplex_dir, "{sample}_{Read}_{postfix}.fastq.gz"),
    params:
        read_list = lambda wildcards:" ".join([os.path.join(wrkdir,tmp_dir,demultiplex_dir, f"{wildcards.sample}", f"{wildcards.sample}_{wildcards.Read}_Split_{split_no}_{wildcards.postfix}.fastq.gz")
                              for split_no in [
                "00" + str(i) if i > 9 else "000" + str(i)
                for i in range(split_count)
            ]])
    threads:
        1
    log:
        os.path.join(wrkdir, log_dir, "merge", "{sample}_{Read}_{postfix}.log")
    resources:
        mem_mb=4000,
        runtime=7*24*60,
        node=1,
    shell:
        "cat {params.read_list} "
        "> {output.read} "
        "&> {log}"